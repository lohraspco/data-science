{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some of my PySpark codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    split,\n",
    "    count,\n",
    "    sum,\n",
    "    avg,\n",
    "    when,\n",
    "    desc,\n",
    "    broadcast,\n",
    ")\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/20 04:57:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/20 04:57:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"dataframPractices\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.option(\"header\", \"true\").csv(\"/data/practice/sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check default partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default partitions: {sales.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition to specific number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After repartition(10): 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "sales_10p = sales.repartition(10)\n",
    "print(f\"After repartition(10): {sales_10p.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After repartition by timestamp: 3\n"
     ]
    }
   ],
   "source": [
    "sales_by_date = sales.repartition(\"timestamp\")\n",
    "print(f\"After repartition by timestamp: {sales_by_date.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "narrow transformation example - doesn't shuffle all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After coalesce(4): 4\n"
     ]
    }
   ],
   "source": [
    "sales_coalesceed = sales_10p.coalesce(4)\n",
    "print(f\"After coalesce(4): {sales_coalesceed.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine partition sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition distribution:\n",
      "Partition 0: 10000 records\n",
      "Partition 1: 10000 records\n",
      "Partition 2: 10000 records\n",
      "Partition 3: 10000 records\n",
      "Partition 4: 10000 records\n",
      "Partition 5: 10000 records\n",
      "Partition 6: 10000 records\n",
      "Partition 7: 10000 records\n",
      "Partition 8: 10000 records\n",
      "Partition 9: 10000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Partition distribution:\")\n",
    "part_sizes = sales_10p.rdd.glom().map(len).collect()\n",
    "for i, size in enumerate(part_sizes):\n",
    "    print(f\"Partition {i}: {size} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformations\n",
    "\n",
    "Check the jobs http://localhost:4041/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter - doesn't require shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High value sales count: 50516\n"
     ]
    }
   ],
   "source": [
    "high_value_sales = sales.filter(col(\"price\").cast(\"double\") > 500)\n",
    "print(f\"High value sales count: {high_value_sales.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map - doesn't require shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+--------+-----+----------+-----------+\n",
      "|order_id|product_id|user_id|quantity|price| timestamp|total_value|\n",
      "+--------+----------+-------+--------+-----+----------+-----------+\n",
      "|       0|       553|   4397|       8|490.6|2023-08-18|     3924.8|\n",
      "|       1|       441|   6066|       2|23.87|2023-10-09|      47.74|\n",
      "+--------+----------+-------+--------+-----+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_with_total = sales.withColumn(\n",
    "    \"total_value\", col(\"quantity\").cast(\"int\") * col(\"price\").cast(\"double\")\n",
    ")\n",
    "sales_with_total.show(2) \n",
    "# sales_with_total.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample - doesn't require shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_sales = sales.sample(fraction=0.1, seed=42)\n",
    "print(f\"Sample size: {sample_sales.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Transformations\n",
    "\n",
    "requires shuffle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSchema:\u001b[0m\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "\u001b[31mNone\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[31mSchema:\\033[0m\")\n",
    "print(f\"\\033[31m{sales.printSchema()}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+------------------+\n",
      "|product_id|order_count|total_quantity|       total_price|\n",
      "+----------+-----------+--------------+------------------+\n",
      "|       691|        117|           676| 527.1941025641026|\n",
      "|       467|         84|           502|476.77404761904756|\n",
      "+----------+-----------+--------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_product = sales_10p.groupBy(\"product_id\").agg(\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    sum(col(\"quantity\").cast(\"int\")).alias(\"total_quantity\"),\n",
    "    avg(col(\"price\").cast(\"double\")).alias(\"total_price\"),\n",
    ")\n",
    "sales_by_product.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.read.option(\"header\", \"true\").csv(\"/data/practice/users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales joined with users:\n",
      "+--------+-------+--------+-------+------+\n",
      "|order_id|user_id|    name|country| price|\n",
      "+--------+-------+--------+-------+------+\n",
      "|   25941|   8950|jkbhyoft| France|949.69|\n",
      "|   13463|   4086|qfgieltd|  India|653.83|\n",
      "|    1143|   9538|gzcpaecn|  China|361.53|\n",
      "|   15590|   1180|zpvjtsye|  India| 27.94|\n",
      "|   11401|   6656|bkojubrf|  China|943.61|\n",
      "+--------+-------+--------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_with_user = sales_10p.join(users, \"user_id\")\n",
    "print(\"Sales joined with users:\")\n",
    "sales_with_user.select(\"order_id\", \"user_id\", \"name\", \"country\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales sorted by price:\n",
      "+--------+----------+-------+--------+------+----------+\n",
      "|order_id|product_id|user_id|quantity| price| timestamp|\n",
      "+--------+----------+-------+--------+------+----------+\n",
      "|   10866|       249|   1438|       9|999.98|2023-06-15|\n",
      "|   24923|       950|    812|       5|999.96|2023-02-06|\n",
      "|   63684|       483|   8612|       7|999.93|2023-08-07|\n",
      "|   52482|       680|   7484|      10|999.91|2023-04-12|\n",
      "|   57854|       975|   5551|       3|999.91|2023-06-01|\n",
      "+--------+----------+-------+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy/sort are wide transformation\n",
    "sorted_sales = sales_10p.orderBy(desc(\"price\"))\n",
    "print(\"Sales sorted by price:\")\n",
    "sorted_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.option(\"header\", \"true\").csv(\"/data/practice/products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast join (optimization for joining with small tables). Products table is small, so we can broadcast it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_prod = sales.join(broadcast(products), \"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching/persisting: Cache a DataFrame we'll use multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_sales = sales.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cached DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sales count (from cache): 100000\n",
      "Average price (from cache): 504.6348144999969\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sales count (from cache): {cached_sales.count()}\")\n",
    "print(\n",
    "    f\"Average price (from cache): {cached_sales.agg({'price': 'avg'}).collect()[0][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "sales_by_month = sales.withColumn(\"month\", split(col(\"timestamp\"), \"-\")[1])\n",
    "sales_by_month.write.partitionBy(\"month\").mode(\"overwrite\").parquet(\n",
    "    \"/data/practice/sales_partitioned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back with partition filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January sales count (using partition pruning): 8462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "january_sales = spark.read.parquet(\"/data/practice/sales_partitioned\").filter(\n",
    "    col(\"month\") == \"01\"\n",
    ")\n",
    "print(f\"January sales count (using partition pruning): {january_sales.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/28 15:01:26 ERROR TaskSchedulerImpl: Lost executor 1 on 172.19.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/03/28 15:01:26 ERROR TaskSchedulerImpl: Lost executor 0 on 172.19.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/03/28 15:01:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_0 !\n",
      "25/03/31 05:06:03 ERROR TaskSchedulerImpl: Lost executor 2 on 172.19.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/03/31 05:06:03 ERROR TaskSchedulerImpl: Lost executor 3 on 172.19.0.5: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/28 12:05:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"hamooon2\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "sc1 = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc1.textFile(\"/data/Common_sense.txt\")\n",
    "rdd1 = rdd.map(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Common', 'Sense'],\n",
       " ['', '', '', '', '']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
