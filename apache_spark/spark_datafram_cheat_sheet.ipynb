{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some of my PySpark codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    split,\n",
    "    count,\n",
    "    sum,\n",
    "    avg,\n",
    "    when,\n",
    "    desc,\n",
    "    broadcast,\n",
    ")\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/20 04:57:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/20 04:57:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"dataframPractices\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 05:58:33 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 05:58:48 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 05:59:03 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 05:59:18 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 05:59:33 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 05:59:48 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:00:03 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:00:18 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:00:33 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:00:48 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:01:03 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:01:18 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:01:33 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/03/20 06:01:48 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "\"\"\"Experiment with partitioning data\"\"\"\n",
    "print(\"\\n=== EXPERIMENT 1: PARTITIONING ===\")\n",
    "\n",
    "# Read sales data\n",
    "sales = spark.read.option(\"header\", \"true\").csv(\"/data/practice/sales.csv\")\n",
    "\n",
    "# Check default partitioning\n",
    "print(f\"Default partitions: {sales.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition to specific number\n",
    "sales_10p = sales.repartition(10)\n",
    "print(f\"After repartition(10): {sales_10p.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition by key\n",
    "sales_by_date = sales.repartition(\"timestamp\")\n",
    "print(f\"After repartition by timestamp: {sales_by_date.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (narrow transformation - doesn't shuffle all data)\n",
    "sales_coalesceed = sales_10p.coalesce(4)\n",
    "print(f\"After coalesce(4): {sales_coalesceed.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Examine partition sizes\n",
    "print(\"Partition distribution:\")\n",
    "part_sizes = sales_10p.rdd.glom().map(len).collect()\n",
    "for i, size in enumerate(part_sizes):\n",
    "    print(f\"Partition {i}: {size} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Demonstrate narrow transformations (no shuffling)\"\"\"\n",
    "print(\"\\n=== EXPERIMENT 2: NARROW TRANSFORMATIONS ===\")\n",
    "\n",
    "# Read sales data\n",
    "sales = spark.read.option(\"header\", \"true\").csv(\"/data/practice/sales.csv\")\n",
    "\n",
    "# Define some narrow transformations\n",
    "print(\"Executing narrow transformations...\")\n",
    "\n",
    "# 1. Filter - doesn't require shuffling\n",
    "high_value_sales = sales.filter(col(\"price\").cast(\"double\") > 500)\n",
    "print(f\"High value sales count: {high_value_sales.count()}\")\n",
    "\n",
    "# 2. Map - doesn't require shuffling\n",
    "sales_with_total = sales.withColumn(\n",
    "    \"total_value\", col(\"quantity\").cast(\"int\") * col(\"price\").cast(\"double\")\n",
    ")\n",
    "sales_with_total.show(5)\n",
    "\n",
    "# 3. Sample - doesn't require shuffling\n",
    "sample_sales = sales.sample(fraction=0.1, seed=42)\n",
    "print(f\"Sample size: {sample_sales.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Demonstrate wide transformations (shuffling required)\"\"\"\n",
    "print(\"\\n=== EXPERIMENT 3: WIDE TRANSFORMATIONS ===\")\n",
    "\n",
    "# Read sales data\n",
    "sales = spark.read.option(\"header\", \"true\").csv(\"/data/practice/sales.csv\")\n",
    "sales = sales.withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
    "sales = sales.withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "print(\"\\033[31mSchema:\\033[0m\")\n",
    "print(f\"\\033[31m{sales.printSchema()}\\033[0m\")\n",
    "\n",
    "# 1. groupBy (wide transformation - requires shuffle)\n",
    "print(\"Executing groupBy (wide transformation)...\")\n",
    "sales_by_product = sales.groupBy(\"product_id\").agg(\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    ")\n",
    "print(\"Sales aggregated by product_id:\")\n",
    "sales_by_product.show(5)\n",
    "\n",
    "# 2. join (wide transformation - requires shuffle)\n",
    "print(\"Executing join (wide transformation)...\")\n",
    "users = spark.read.option(\"header\", \"true\").csv(\"/data/practice/users.csv\")\n",
    "\n",
    "# Join sales with users - this is a wide transformation\n",
    "sales_with_user = sales.join(users, \"user_id\")\n",
    "print(\"Sales joined with users:\")\n",
    "sales_with_user.select(\"order_id\", \"user_id\", \"name\", \"country\", \"price\").show(5)\n",
    "\n",
    "# 3. orderBy/sort (wide transformation - requires shuffle)\n",
    "print(\"Executing orderBy (wide transformation)...\")\n",
    "sorted_sales = sales.orderBy(desc(\"price\"))\n",
    "print(\"Sales sorted by price:\")\n",
    "sorted_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Demonstrate various optimizations for distributed processing\"\"\"\n",
    "print(\"\\n=== EXPERIMENT 4: OPTIMIZATIONS ===\")\n",
    "\n",
    "sales = spark.read.option(\"header\", \"true\").csv(\"/data/practice/sales.csv\")\n",
    "users = spark.read.option(\"header\", \"true\").csv(\"/data/practice/users.csv\")\n",
    "products = spark.read.option(\"header\", \"true\").csv(\"/data/practice/products.csv\")\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "sales = sales.withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
    "sales = sales.withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "\n",
    "# 1. Broadcast join (optimization for joining with small tables)\n",
    "print(\"Demonstrating broadcast join...\")\n",
    "# Products table is small, so we can broadcast it\n",
    "sales_with_product = sales.join(broadcast(products), \"product_id\")\n",
    "print(\"Executed broadcast join - joining sales with products\")\n",
    "\n",
    "# 2. Caching/persisting\n",
    "print(\"Demonstrating caching...\")\n",
    "# Cache a DataFrame we'll use multiple times\n",
    "cached_sales = sales.cache()\n",
    "\n",
    "# Use the cached DataFrame\n",
    "print(f\"Total sales count (from cache): {cached_sales.count()}\")\n",
    "print(\n",
    "    f\"Average price (from cache): {cached_sales.agg({'price': 'avg'}).collect()[0][0]}\"\n",
    ")\n",
    "\n",
    "# 3. Partition pruning\n",
    "print(\"Demonstrating partition pruning...\")\n",
    "# Partition data by timestamp (simulating a common pattern)\n",
    "sales_by_month = sales.withColumn(\"month\", split(col(\"timestamp\"), \"-\")[1])\n",
    "sales_by_month.write.partitionBy(\"month\").mode(\"overwrite\").parquet(\n",
    "    \"/data/practice/sales_partitioned\"\n",
    ")\n",
    "\n",
    "# Read back with partition filtering\n",
    "january_sales = spark.read.parquet(\"/data/practice/sales_partitioned\").filter(\n",
    "    col(\"month\") == \"01\"\n",
    ")\n",
    "print(f\"January sales count (using partition pruning): {january_sales.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
