{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main types of Recommender Systems:\n",
    "\n",
    "1. Collaborative Filtering: Based on user-item interactions.\n",
    "- User-based filtering: Recommends items liked by similar users.\n",
    "- Item-based filtering: Recommends similar items to what a user has interacted with.\n",
    "Techniques: k-Nearest Neighbors (k-NN), Matrix Factorization (SVD, ALS).\n",
    "2. Content-Based Filtering: Recommends items based on item features (e.g., genre, description).\n",
    "- Uses TF-IDF, word embeddings (e.g., Word2Vec, BERT), or deep learning.\n",
    "3. Hybrid Approaches: Combines collaborative and content-based filtering.\n",
    "Example: Netflix recommends movies based on both user preferences and movie metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/bitnami/python/bin/python3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the ipykernel to the jupyter running in your spark-master node\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, posexplode, col\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    StructType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/24 04:01:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"moviesOneMillion\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "Data is shared with the docker through shared volume mapping (data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|   1193|     5|978300760|\n",
      "|     1|    661|     3|978302109|\n",
      "+------+-------+------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 13:43:31 ERROR TaskSchedulerImpl: Lost executor 1 on 172.19.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/03/24 13:43:31 ERROR TaskSchedulerImpl: Lost executor 0 on 172.19.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/03/25 16:40:55 ERROR TaskSchedulerImpl: Lost executor 2 on 172.19.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/03/25 16:40:55 ERROR TaskSchedulerImpl: Lost executor 3 on 172.19.0.5: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "rating_schema = StructType([StructField(\"userId\", IntegerType(), False), \n",
    "                            StructField(\"movieId\", IntegerType(), True), \n",
    "                            StructField(\"rating\" , IntegerType(), True),\n",
    "                            StructField(\"timestamp\", IntegerType(), True)])\n",
    "rating = spark.read.csv(\"/data/ml-1m/ratings.dat\", sep=\"::\", schema=rating_schema)\n",
    "\n",
    "rating.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------------+\n",
      "| id|           title|              genres|\n",
      "+---+----------------+--------------------+\n",
      "|  1|Toy Story (1995)|Animation|Childre...|\n",
      "|  2|  Jumanji (1995)|Adventure|Childre...|\n",
      "+---+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"genres\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "movies = spark.read.csv(f\"/data/ml-1m/movies.dat\", sep=\"::\", schema=movies_schema)\n",
    "movies.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+----------+-----+\n",
      "| id|gender|age|occupation|  zip|\n",
      "+---+------+---+----------+-----+\n",
      "|  1|     F|  1|        10|48067|\n",
      "|  2|     M| 56|        16|70072|\n",
      "+---+------+---+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "users_schema = StructType([StructField(\"id\", IntegerType(), False), \n",
    "                           StructField(\"gender\", StringType(), True),\n",
    "                           StructField(\"age\", IntegerType(), True),\n",
    "                           StructField(\"occupation\", StringType(), True),\n",
    "                           StructField(\"zip\", IntegerType(), True)])\n",
    "users = spark.read.schema(schema=users_schema).csv(\n",
    "    f\"/data/ml-1m//users.dat\", sep=\"::\", schema=users_schema)\n",
    "users.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS Recommender\n",
    "ALS aims to find two matrices, \n",
    "- User Matrix (U): Represents users as latent (hidden) feature vectors.\n",
    "- Item Matrix (V): Represents items as latent feature vectors.</br>\n",
    "their product (U * P) approximates the original user-item rating matrix (R) </br>\n",
    "ALS fixes one matrix (e.g., U) and solves for the other (V), then alternates. \n",
    "ALS minimizes the error between predicted ratings (U × Vᵀ) and actual ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = rating.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als = ALS(\n",
    "    userCol=\"userId\", \n",
    "    itemCol=\"movieId\", \n",
    "    ratingCol=\"rating\", \n",
    "    maxIter=10, # more iterations, more accurate, but slower and more prone to overfitting\n",
    "    regParam=0.1, # regularization parameter to prevent overfitting\n",
    "    rank=10, # number of latent factors\n",
    "    coldStartStrategy=\"drop\" # drop users and items with less than 20 ratings\n",
    ")\n",
    "\n",
    "model = als.fit(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8683\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 429:================================================>     (90 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------------------+---------------+----------------------+---------------+----------------------+\n",
      "|userId|recommendation1|recommendation1_rating|recommendation2|recommendation2_rating|recommendation3|recommendation3_rating|\n",
      "+------+---------------+----------------------+---------------+----------------------+---------------+----------------------+\n",
      "|     1|           3233|              4.598261|            128|             4.5023494|            527|             4.4881306|\n",
      "|    12|           2309|             4.6853147|            598|             4.4963436|           1039|             4.4958973|\n",
      "|    22|           2309|              4.366799|            989|             4.0954137|           1169|               4.07395|\n",
      "+------+---------------+----------------------+---------------+----------------------+---------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs = model.recommendForAllUsers(3)\n",
    "user_recs.select(\n",
    "    \"userId\",\n",
    "    col(\"recommendations\")[0][\"movieId\"].alias(\"recommendation1\"),\n",
    "    col(\"recommendations\")[0][\"rating\"].alias(\"recommendation1_rating\"),\n",
    "    col(\"recommendations\")[1][\"movieId\"].alias(\"recommendation2\"),\n",
    "    col(\"recommendations\")[1][\"rating\"].alias(\"recommendation2_rating\"),\n",
    "    col(\"recommendations\")[2][\"movieId\"].alias(\"recommendation3\"),\n",
    "    col(\"recommendations\")[2][\"rating\"].alias(\"recommendation3_rating\"),\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 576:==============================================>       (86 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+---------+\n",
      "|userId|pos|movieId|   rating|\n",
      "+------+---+-------+---------+\n",
      "|     1|  0|   3233| 4.598261|\n",
      "|     1|  1|    128|4.5023494|\n",
      "|     1|  2|    527|4.4881306|\n",
      "|    12|  0|   2309|4.6853147|\n",
      "|    12|  1|    598|4.4963436|\n",
      "|    12|  2|   1039|4.4958973|\n",
      "+------+---+-------+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs_ex = user_recs.select(\n",
    "    \"userId\",\n",
    "    posexplode(\"recommendations\").alias(\"pos\", \"rec\")\n",
    ").select(\"userId\", \"pos\", col(\"rec.movieId\").alias(\"movieId\"), col(\"rec.rating\").alias(\"rating\"))\n",
    "user_recs_ex.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+---------+----+--------------------+-----------+\n",
      "|userId|pos|movieId|   rating|  id|               title|     genres|\n",
      "+------+---+-------+---------+----+--------------------+-----------+\n",
      "|     1|  0|   3233| 4.598261|3233|Smashing Time (1967)|     Comedy|\n",
      "|     1|  1|    128|4.5023494| 128|Jupiter's Wife (1...|Documentary|\n",
      "|     1|  2|    527|4.4881306| 527|Schindler's List ...|  Drama|War|\n",
      "|    12|  0|   2309|4.6853147|2309|Inheritors, The (...|      Drama|\n",
      "|    12|  1|    598|4.4963436| 598|Window to Paris (...|     Comedy|\n",
      "|    12|  2|   1039|4.4958973|1039|Synthetic Pleasur...|Documentary|\n",
      "+------+---+-------+---------+----+--------------------+-----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_rec_with_movie_names = user_recs_ex.join(\n",
    "    broadcast(movies), # each node performs operations locally using broadcasted DF without shuffle data across the network\n",
    "    col(\"movieId\") == col(\"id\"), \"inner\"\n",
    ")\n",
    "user_rec_with_movie_names.show(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
