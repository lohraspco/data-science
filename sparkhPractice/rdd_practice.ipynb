{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction\n",
    "\n",
    "\n",
    "In this Jupyter Notebook you will find materials about \n",
    "1. <span style=\"color:red\">RDD</span>   \n",
    "- Reading a text file (from local or HDFS)\n",
    "- map() and flatMap\n",
    "- reduceByKey(), groupByKey(), sortByKey(), keys(), and values()\n",
    "- join(), rightOuterJoin(), leftOuterJoin(), cogroup(), subtractByKey()\n",
    "- with key/value data, use mapValues() and flatMapValues() of your transformation doesn't affect the keys. It is more efficient because it allows spark to maintain the same partitioning as original RDD instead of shuffling data.\n",
    "- filter()\n",
    "* Question: am I modifying the keys: yes then use map and flatMap, no then use mapValues and flatMapValues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some hints\n",
    "\n",
    "max for each item: reduceByKey(lambda x,y:max(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-12-28 22:35:21,842 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-12-28 22:35:22,408 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"rdd_practice\").getOrCreate()\n",
    "sc  = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '35015'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/hadoop/lohrasp/analyticsoptim/spark-warehouse'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'master'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1640730921773'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1640730922566'),\n",
       " ('spark.app.name', 'rdd_practice'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of stopwords to be removed from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "\n",
    "for k in [\"movies\",\"ratings\",\"users\"]:\n",
    "    d[k] = sc.textFile(f\"hdfs:///user/hadoop/moviesdb/ml-1m/{k}.dat\").map(lambda li:li.split(\"::\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '1193', '5', '978300760'], ['1', '661', '3', '978302109']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " d[\"ratings\"].take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'5': 226310, '3': 261197, '4': 348971, '2': 107557, '1': 56174})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = d[\"ratings\"].map(lambda x:x[2]).countByValue()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4', 348971), ('1', 56174), ('5', 226310), ('3', 261197), ('2', 107557)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating2 = d[\"ratings\"].map(lambda x: (x[2], 1))\n",
    "rating2syn = rating2.reduceByKey(lambda x, y: x+y)\n",
    "rating2syn.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5', '1193'), ('3', '661')] [('5', ('1193', 1)), ('3', ('661', 1))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4', 1875.5138793767962), ('1', 1972.758838608609), ('5', 1728.2636781406036), ('3', 1918.5037423860151), ('2', 1937.4035348698828)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4', (654499954, 348971)),\n",
       " ('1', (110817755, 56174)),\n",
       " ('5', (391123353, 226310)),\n",
       " ('3', (501107422, 261197)),\n",
       " ('2', (208381312, 107557))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingKV = d[\"ratings\"].map(lambda x:(x[2],x[1]))\n",
    "rating3 = ratingKV.mapValues(lambda x: (x,1)).reduceByKey(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n",
    "averagePerRating = rating3.mapValues(lambda x:x[0]/x[1])\n",
    "print(ratingKV.take(2),ratingKV.mapValues(lambda x: (x,1)).take(2))\n",
    "print(averagePerRating.collect())\n",
    "rating3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from HDFS\n",
    "\n",
    " On the Origin of Species, by Charles Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rdd1 = sc.textFile(\n",
    "    \"hdfs:///user/hadoop/OntheOriginofSpecies.txt\").flatMap(lambda text: re.compile(r'\\W',re.UNICODE).split(text.lower()))\n",
    "rdd1 = rdd1.filter(lambda x: x not in stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj ['project', 'project', 'project']\n",
      "gute ['gutenberg', 'gutenberg', 'gutenberg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.groupBy(lambda x:x[:4])\n",
    "for k , v in rdd2.take(2):\n",
    "    print(k,list(v)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'title'), (1, '1st')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def swapTuple(t):\n",
    "    return (t[1],t[0])\n",
    "numOccurance = rdd1.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).map(swapTuple).sortByKey()\n",
    "numOccurance.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj ['project', 'projecting']\n",
      "gute ['gutenberg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd1.distinct()\n",
    "rdd4 = rdd3.groupBy(lambda x:x[:4])\n",
    "for k , v in rdd4.take(2):\n",
    "    print(k,list(v)[:3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
